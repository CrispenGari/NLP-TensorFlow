{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baking-enzyme",
   "metadata": {},
   "source": [
    "### Category and Sentiment Classification.\n",
    "\n",
    "In this notebook we are going to expand the previous 2 Notebooks to create a `NN` that will be able to predict the class which s review belongs to as well as the sentiment if it is posive or negative. So we are going to create a model that will take one input and output two outputs suing the flexible keras Functional API.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "elect-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json, re, random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-greeting",
   "metadata": {},
   "source": [
    "### Data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-portland",
   "metadata": {},
   "source": [
    "We have 5 categories and each categories which are:\n",
    "\n",
    "    * Books\n",
    "    * Clothing\n",
    "    * Grocery\n",
    "    * Patio\n",
    " \n",
    "Based on the review we want to be able to predict which category does the review belongs to.\n",
    "\n",
    "We also have reviews that start from 0 to 5. We are going to assume that greater than 3 is a positive (1) otherwise negative (0)\n",
    "\n",
    "### File structure\n",
    "\n",
    "```\n",
    "files\n",
    "    category\n",
    "        Books_small.json\n",
    "        Clothing_small.json\n",
    "        Electronics_small.json\n",
    "        Grocery_small.json\n",
    "        Patio_small.json\n",
    "```\n",
    "Each json file has the following structure\n",
    "\n",
    "```json\n",
    "{\n",
    "     \"reviewerID\": \"..\", \n",
    "     \"asin\": \"..\", \n",
    "     \"reviewerName\": \"..\"\",\n",
    "     \"helpful\": [..],\n",
    "     \"reviewText\": \"...\",\n",
    "     \"overall\": .., \n",
    "     \"summary\": \"..\", \n",
    "     \"unixReviewTime\": ..., \n",
    "     \"reviewTime\": \"..\"\n",
    " }\n",
    "```\n",
    "### Labels.\n",
    "\n",
    "```\n",
    "[positive = 1, negative = 0]\n",
    "\n",
    "[BOOKS = 0, CLOTHING=1, ELECTRONICS=2, GROCERY=3, PATIO=4]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nonprofit-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review:\n",
    "    def __init__(self, review, sentiment, category):\n",
    "        self.category = category\n",
    "        self.review = review\n",
    "        self.sentiment = sentiment\n",
    "\n",
    "    def get_sentiment(sentiment):\n",
    "        return 0 if sentiment < 3 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-buyer",
   "metadata": {},
   "source": [
    "### Creating preprocessing function that remove Numbers and double spacing in a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "respiratory-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sent):\n",
    "    a = re.sub(r'\\d', ' ', sent)\n",
    "    b = re.sub(r'\\s+', ' ', a)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mediterranean-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"./files/category/\"\n",
    "test_text_reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-finger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "contained-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for cat, category in enumerate(os.listdir(base_url)):\n",
    "    with open(os.path.join(base_url, category), 'r') as f:\n",
    "        seed = random.randrange(0, 500)\n",
    "        for index, line in enumerate(f):\n",
    "            category_json = json.loads(line)\n",
    "            data.append(Review(\n",
    "                clean_text(category_json[\"reviewText\"]),\n",
    "                Review.get_sentiment(category_json[\"overall\"]),\n",
    "                cat\n",
    "            ))\n",
    "            if index == seed:\n",
    "                test_text_reviews.append(category_json[\"reviewText\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "musical-colombia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Da Silva takes the divine by storm with this unique new novel. She develops a world unlike any others while keeping it firmly in the real world. This is a very well written and entertaining novel. I was quite impressed and intrigued by the way that this solid storyline was developed, bringing the readers right into the world of the story. I was engaged throughout and definitely enjoyed my time spent reading it.I loved the character development in this novel. Da Silva creates a cast of high school students who actually act like high school students. I really appreciated the fact that none of them were thrown into situations far beyond their years, nor did they deal with events as if they had decades of life experience under their belts. It was very refreshing and added to the realism and impact of the novel. The friendships between the characters in this novel were also truly touching.Overall, this novel was fantastic. I can&# ;t wait to read more and to find out what happens next in the series. I&# ;d definitely recommend this debut novel by Da Silva to those who want a little YA fun with a completely unique & shocking storyline.Please note that I received a complimentary copy of this work in exchange for an honest review.',\n",
       " 0,\n",
       " 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].review, data[0].category, data[0].sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beautiful-beauty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(test_text_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "other-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_text = [i.review for i in data]\n",
    "reviews_category_labels = [i.category for i in data]\n",
    "reviews_sentiment_labels = [i.sentiment for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "planned-accident",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000}),\n",
       " Counter({1: 4592, 0: 408}))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(reviews_category_labels), Counter(reviews_sentiment_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-runner",
   "metadata": {},
   "source": [
    "### Vocabulary size `aka` number of unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "floppy-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for sent in reviews_text:\n",
    "    words = word_tokenize(sent)\n",
    "    for word in words:\n",
    "        counter[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adequate-hierarchy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 24101), ('the', 19797), (',', 16566)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "answering-subscription",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27770"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(counter)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-nigeria",
   "metadata": {},
   "source": [
    "> We have `~28k`unique words in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-plenty",
   "metadata": {},
   "source": [
    "### Now, Creating word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "suitable-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "statistical-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
    "tokenizer.fit_on_texts(reviews_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "biblical-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indices = tokenizer.word_index\n",
    "word_indices_reversed = dict([(v, k) for (k, v) in word_indices.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-kuwait",
   "metadata": {},
   "source": [
    "### A function that converts `sequences to sents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "considerable-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_text(seq):\n",
    "    return \" \".join([word_indices_reversed[i] for i in seq])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-military",
   "metadata": {},
   "source": [
    "### A function that converts `sents to sequences`.\n",
    "We are going to use this function during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "suspended-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_sequence(sent):\n",
    "    words = word_tokenize(str(sent).lower())\n",
    "    sequences = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            sequences.append(word_indices[word])\n",
    "        except:\n",
    "            sequences.append(0)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-valuation",
   "metadata": {},
   "source": [
    "### Loading pretrainned weights `glove.6B.`\n",
    "We are going to use this weights in our `embedding` layer which is the first layer in the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "familiar-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "with open(r\"C:\\Users\\crisp\\Downloads\\glove.6B\\glove.6B.100d.txt\", encoding='utf8') as glove_file:\n",
    "    for line in glove_file:\n",
    "        records = line.split()\n",
    "        word  = records[0]\n",
    "        vectors = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary[word] = vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-biography",
   "metadata": {},
   "source": [
    "> Creating an `embedding` matrix that suits our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "stable-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    vector = embeddings_dictionary.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[index] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-clock",
   "metadata": {},
   "source": [
    "### Creating sequences from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "choice-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_tokens = tokenizer.texts_to_sequences(reviews_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "convenient-messaging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5840, 6775, 374, 1, 6776, 74, 2636, 13, 9, 1136, 151, 444, 78, 5155, 4, 500, 981, 88, 357, 130, 724, 6, 3355, 10, 1, 363, 500, 9, 8, 4, 27, 47, 431, 3, 2033, 444, 2, 20, 216, 775, 3, 2637, 74, 1, 90, 12, 9, 476, 1256, 20, 1704, 3591, 1, 927, 141, 91, 1, 500, 7, 1, 100, 2, 20, 3592, 1198, 3, 230, 355, 14, 68, 1061, 226, 6, 2, 237, 1, 566, 2118, 10, 9, 444, 5840, 6775, 3593, 4, 2034, 7, 218, 904, 3139, 122, 241, 2795, 26, 218, 904, 3139, 2, 57, 2514, 1, 411, 12, 1315, 7, 35, 83, 2388, 91, 2119, 196, 1257, 109, 162, 999, 99, 24, 460, 13, 1890, 22, 30, 24, 46, 3594, 7, 203, 501, 351, 109, 4208, 6, 20, 27, 2291, 3, 545, 5, 1, 11053, 3, 2638, 7, 1, 444, 1, 3873, 318, 1, 214, 10, 9, 444, 83, 72, 795, 2958, 292, 9, 444, 20, 776, 2, 36, 238, 428, 5, 80, 39, 3, 5, 143, 43, 58, 951, 251, 10, 1, 194, 2, 384, 230, 124, 9, 5156, 444, 74, 5840, 6775, 5, 197, 122, 114, 4, 70, 2959, 487, 13, 4, 609, 1136, 3595, 1256, 894, 610, 12, 2, 368, 4, 4209, 1164, 7, 9, 98, 10, 1081, 11, 51, 599, 211]\n"
     ]
    }
   ],
   "source": [
    "print(sequence_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "complex-algebra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'da silva takes the divine by storm with this unique new novel she develops a world unlike any others while keeping it firmly in the real world this is a very well written and entertaining novel i was quite impressed and intrigued by the way that this solid storyline was developed bringing the readers right into the world of the story i was engaged throughout and definitely enjoyed my time spent reading it i loved the character development in this novel da silva creates a cast of high school students who actually act like high school students i really appreciated the fact that none of them were thrown into situations far beyond their years nor did they deal with events as if they had decades of life experience under their belts it was very refreshing and added to the realism and impact of the novel the friendships between the characters in this novel were also truly touching overall this novel was fantastic i can t wait to read more and to find out what happens next in the series i d definitely recommend this debut novel by da silva to those who want a little ya fun with a completely unique shocking storyline please note that i received a complimentary copy of this work in exchange for an honest review'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_text(sequence_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-grade",
   "metadata": {},
   "source": [
    "### Padding sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "choice-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "consecutive-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 100\n",
    "sequences_padded = pad_sequences(sequence_tokens, maxlen=max_words, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-iraqi",
   "metadata": {},
   "source": [
    "### Creating a model.\n",
    "\n",
    "### Model `Achitecture` Functional API.\n",
    "\n",
    "```\n",
    "                    [ Input ]\n",
    "                        |\n",
    "                        |\n",
    "                [ Embedding Layer]\n",
    "                        |\n",
    "                        |\n",
    "[ LSTM ] <---- [Bidirectional Layer] ----> [GRU] (forward_layer)\n",
    " (backward_layer)       |\n",
    "                        |\n",
    "                 [ Flatten Layer]\n",
    "                        |\n",
    "                        |\n",
    "                 [Dense Layer 1]\n",
    "                        |\n",
    "                        |    \n",
    "                 [Dense Layer 2]\n",
    "                        |\n",
    "                        |\n",
    "            |--------------------------|\n",
    "            |                          |\n",
    "      [Dense Layer 3]            [Dense Layer 4]\n",
    "        (pos, neg)       (book, electronic, clothing, grocery, patio)            \n",
    "      \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "apart-voluntary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"amazon_sentiment_category_classifier\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer (InputLayer)        [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer (Embedding)     (None, 100, 100)     2777000     input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_layer (Bidirectio (None, 100, 128)     74112       embedding_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_layer (Flatten)         (None, 12800)        0           bidirectional_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 64)           819264      flatten_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64)           0           fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 512)          33280       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sentiment_output (Dense)        (None, 1)            513         fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "category_output (Dense)         (None, 5)            2565        fc_2[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 3,706,734\n",
      "Trainable params: 929,734\n",
      "Non-trainable params: 2,777,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "forward_layer = keras.layers.GRU(64, return_sequences=True, dropout=.5 )\n",
    "backward_layer = keras.layers.LSTM(64, activation='relu', return_sequences=True,\n",
    "                       go_backwards=True, dropout=.5)\n",
    "\n",
    "input_layer = keras.layers.Input(shape=(100, ), name=\"input_layer\")\n",
    "\n",
    "embedding_layer = keras.layers.Embedding(\n",
    "                        vocabulary_size, 100, \n",
    "                        input_length=max_words, \n",
    "                        weights=[embedding_matrix], \n",
    "                        trainable=False, name=\"embedding_layer\"\n",
    "                    )(input_layer)\n",
    "\n",
    "bidirectional_layer = keras.layers.Bidirectional(\n",
    "                                forward_layer,\n",
    "                                backward_layer = backward_layer,\n",
    "                                name =\"bidirectional_layer\"\n",
    "                            )(embedding_layer)\n",
    "\n",
    "flatten_layer = keras.layers.Flatten(name=\"flatten_layer\")(bidirectional_layer)\n",
    "fc_1 = keras.layers.Dense(64, activation='relu', name=\"fc_1\")(flatten_layer)\n",
    "dropout_layer = keras.layers.Dropout(.3, name=\"dropout\")(fc_1)\n",
    "fc_2 = keras.layers.Dense(512, activation='relu', name='fc_2')(dropout_layer)\n",
    "\n",
    "# Output layers\n",
    "\n",
    "sentiment_output = keras.layers.Dense(1, activation='sigmoid', name='sentiment_output')(fc_2)\n",
    "category_output = keras.layers.Dense(5, activation='softmax', name='category_output')(fc_2)\n",
    "\n",
    "model = keras.Model(inputs=input_layer, outputs=[sentiment_output, category_output], name=\"amazon_sentiment_category_classifier\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-topic",
   "metadata": {},
   "source": [
    "### Label processing.\n",
    "* `reviews_category_labels` - should be one hot vectors\n",
    "\n",
    "* `reviews_sentiment_labels` - should be just a single number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "broadband-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_category_labels_one_hot = tf.one_hot(reviews_category_labels, depth=5).numpy().astype('float32')\n",
    "reviews_sentiment_labels_binary = np.array(reviews_sentiment_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-southeast",
   "metadata": {},
   "source": [
    "### Splitting and shuffling Labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dress-poster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4750, 100), (250, 100), (4750,), (250,), (4750, 5), (250, 5))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text_train, review_text_test, reviews_sentiment_labels_train, reviews_sentiment_labels_test,  reviews_category_labels_train, reviews_category_labels_test = train_test_split(\n",
    "   sequences_padded,  reviews_sentiment_labels_binary, reviews_category_labels_one_hot,\n",
    "  random_state = 42,\n",
    " test_size = .05\n",
    ") \n",
    "\n",
    "review_text_train.shape, review_text_test.shape, reviews_sentiment_labels_train.shape, reviews_sentiment_labels_test.shape,  reviews_category_labels_train.shape, reviews_category_labels_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-childhood",
   "metadata": {},
   "source": [
    "### Trainning the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "private-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stoping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_sentiment_output_loss',\n",
    "    min_delta=0,\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "common-murder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "119/119 [==============================] - 30s 132ms/step - loss: 1.7951 - sentiment_output_loss: 0.3277 - category_output_loss: 1.4673 - sentiment_output_accuracy: 0.8955 - category_output_accuracy: 0.3432 - val_loss: 0.7533 - val_sentiment_output_loss: 0.3308 - val_category_output_loss: 0.4225 - val_sentiment_output_accuracy: 0.9105 - val_category_output_accuracy: 0.8474\n",
      "Epoch 2/10\n",
      "119/119 [==============================] - 15s 123ms/step - loss: 0.8115 - sentiment_output_loss: 0.2874 - category_output_loss: 0.5241 - sentiment_output_accuracy: 0.9196 - category_output_accuracy: 0.8210 - val_loss: 0.6394 - val_sentiment_output_loss: 0.3015 - val_category_output_loss: 0.3379 - val_sentiment_output_accuracy: 0.9105 - val_category_output_accuracy: 0.8853\n",
      "Epoch 3/10\n",
      "119/119 [==============================] - 14s 118ms/step - loss: 0.5834 - sentiment_output_loss: 0.2485 - category_output_loss: 0.3349 - sentiment_output_accuracy: 0.9301 - category_output_accuracy: 0.8899 - val_loss: 0.5668 - val_sentiment_output_loss: 0.3011 - val_category_output_loss: 0.2657 - val_sentiment_output_accuracy: 0.9105 - val_category_output_accuracy: 0.9147\n",
      "Epoch 4/10\n",
      "119/119 [==============================] - 14s 120ms/step - loss: 0.5585 - sentiment_output_loss: 0.2709 - category_output_loss: 0.2877 - sentiment_output_accuracy: 0.9133 - category_output_accuracy: 0.9064 - val_loss: 0.6599 - val_sentiment_output_loss: 0.3108 - val_category_output_loss: 0.3491 - val_sentiment_output_accuracy: 0.9105 - val_category_output_accuracy: 0.8926\n",
      "Epoch 5/10\n",
      "119/119 [==============================] - 14s 118ms/step - loss: 0.4636 - sentiment_output_loss: 0.2308 - category_output_loss: 0.2328 - sentiment_output_accuracy: 0.9239 - category_output_accuracy: 0.9180 - val_loss: 0.5691 - val_sentiment_output_loss: 0.2923 - val_category_output_loss: 0.2768 - val_sentiment_output_accuracy: 0.9095 - val_category_output_accuracy: 0.9095\n",
      "Epoch 6/10\n",
      "119/119 [==============================] - 14s 117ms/step - loss: 0.4139 - sentiment_output_loss: 0.2176 - category_output_loss: 0.1963 - sentiment_output_accuracy: 0.9191 - category_output_accuracy: 0.9350 - val_loss: 0.6493 - val_sentiment_output_loss: 0.3152 - val_category_output_loss: 0.3341 - val_sentiment_output_accuracy: 0.9084 - val_category_output_accuracy: 0.9074\n",
      "Epoch 7/10\n",
      "119/119 [==============================] - 14s 117ms/step - loss: 0.3351 - sentiment_output_loss: 0.1795 - category_output_loss: 0.1556 - sentiment_output_accuracy: 0.9313 - category_output_accuracy: 0.9497 - val_loss: 0.6773 - val_sentiment_output_loss: 0.3448 - val_category_output_loss: 0.3325 - val_sentiment_output_accuracy: 0.9095 - val_category_output_accuracy: 0.8968\n",
      "Epoch 8/10\n",
      "119/119 [==============================] - 14s 117ms/step - loss: 0.3402 - sentiment_output_loss: 0.1746 - category_output_loss: 0.1656 - sentiment_output_accuracy: 0.9282 - category_output_accuracy: 0.9368 - val_loss: 0.7221 - val_sentiment_output_loss: 0.3961 - val_category_output_loss: 0.3260 - val_sentiment_output_accuracy: 0.9105 - val_category_output_accuracy: 0.9063\n",
      "Epoch 9/10\n",
      "119/119 [==============================] - 14s 117ms/step - loss: 0.2724 - sentiment_output_loss: 0.1468 - category_output_loss: 0.1257 - sentiment_output_accuracy: 0.9402 - category_output_accuracy: 0.9525 - val_loss: 0.7790 - val_sentiment_output_loss: 0.4175 - val_category_output_loss: 0.3614 - val_sentiment_output_accuracy: 0.9095 - val_category_output_accuracy: 0.9021\n",
      "Epoch 10/10\n",
      "119/119 [==============================] - 14s 117ms/step - loss: 0.2509 - sentiment_output_loss: 0.1196 - category_output_loss: 0.1313 - sentiment_output_accuracy: 0.9537 - category_output_accuracy: 0.9554 - val_loss: 0.9199 - val_sentiment_output_loss: 0.4863 - val_category_output_loss: 0.4337 - val_sentiment_output_accuracy: 0.9095 - val_category_output_accuracy: 0.9084\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss = {\n",
    "        \"sentiment_output\" : keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        \"category_output\" : keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    },\n",
    "    metrics = ['accuracy'],\n",
    "    optimizer = keras.optimizers.Adam()\n",
    ")\n",
    "history = model.fit(\n",
    "    review_text_train, \n",
    "    y = [reviews_sentiment_labels_train, reviews_category_labels_train],\n",
    "    epochs = 10,\n",
    "    verbose = 1,\n",
    "    validation_split = .2,\n",
    "    shuffle=True,\n",
    "    batch_size= 32,\n",
    "    validation_batch_size = 16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-zimbabwe",
   "metadata": {},
   "source": [
    "### Evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "stainless-stereo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 124ms/step - loss: 0.9556 - sentiment_output_loss: 0.3524 - category_output_loss: 0.6032 - sentiment_output_accuracy: 0.9320 - category_output_accuracy: 0.8960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.955642819404602,\n",
       " 0.3524135649204254,\n",
       " 0.6032292246818542,\n",
       " 0.9319999814033508,\n",
       " 0.8960000276565552]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate( review_text_test, \n",
    "                y = [reviews_sentiment_labels_test, reviews_category_labels_test],\n",
    "                verbose=1, batch_size=128\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-button",
   "metadata": {},
   "source": [
    "### Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "auburn-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sent):\n",
    "    sentiments = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "    categories =[\"BOOKS\", \"CLOTHES\", \"ELECTRONICS\", \"GROCERY\", \"PATIO\"]\n",
    "    tokens = sent_to_sequence(sent)\n",
    "    padded_tokens = pad_sequences([tokens], maxlen=max_words, padding=\"post\", truncating=\"post\")\n",
    "    sentiment_prediction, category_prediction = model(padded_tokens)\n",
    "    \n",
    "    sentiment_prediction = tf.squeeze(tf.round(sentiment_prediction)).numpy().astype('int32')\n",
    "    category_prediction = tf.argmax(category_prediction, axis=1).numpy()[0]\n",
    "    \n",
    "    \n",
    "    print(f'Predicted Classes:\\t [{sentiment_prediction}, {category_prediction}]\\nPredicted Labels:\\t[{sentiments[sentiment_prediction]}, {categories[category_prediction]}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "gentle-dominican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes:\t [1, 1]\n",
      "Predicted Labels:\t[POSITIVE, CLOTHES]\n"
     ]
    }
   ],
   "source": [
    "predict(test_text_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adaptive-boulder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes:\t [1, 1]\n",
      "Predicted Labels:\t[POSITIVE, CLOTHES]\n"
     ]
    }
   ],
   "source": [
    "predict(test_text_reviews[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fatty-malawi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes:\t [1, 2]\n",
      "Predicted Labels:\t[POSITIVE, ELECTRONICS]\n"
     ]
    }
   ],
   "source": [
    "predict(test_text_reviews[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "excessive-commission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes:\t [1, 3]\n",
      "Predicted Labels:\t[POSITIVE, GROCERY]\n"
     ]
    }
   ],
   "source": [
    "predict(test_text_reviews[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "criminal-blend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes:\t [1, 4]\n",
      "Predicted Labels:\t[POSITIVE, PATIO]\n"
     ]
    }
   ],
   "source": [
    "predict(test_text_reviews[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-wheat",
   "metadata": {},
   "source": [
    "### A negative Patio Review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "seasonal-merchandise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes:\t [1, 4]\n",
      "Predicted Labels:\t[POSITIVE, PATIO]\n"
     ]
    }
   ],
   "source": [
    "predict([\"There has been relatively significant traffic of moles under the soil in my yard. I have tried a wide variety of repellants, traps, and poisons for moles, but not all of them have worked, but this one did not work well. I am not sure if the product was too diluted or it simply did not work. If this product is good, then why is it that the manufacturer discontinued this product?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-tourist",
   "metadata": {},
   "source": [
    "## The model is not performing well on sentiment classification due to few negative reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
