{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_CovidTweets_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT5TU5IL7PlG"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-Xgp5qZ6rsB"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import csv, json, time\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_R85uaq9Lc1"
      },
      "source": [
        "### Mounting the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi9xrHhg9KjX",
        "outputId": "8b458f57-97a8-4b94-b5f0-710773267614"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9c5dvVn6rpl",
        "outputId": "7d659cdf-80e2-4c9b-b8c7-3bda2aec0815"
      },
      "source": [
        "data_path = '/content/drive/MyDrive/NLP Data/corona-tweets'\n",
        "os.path.exists(data_path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GgcG6uztxsM"
      },
      "source": [
        "data_frame = pd.read_csv(os.path.join(data_path, 'Corona_NLP_test.csv'))\n",
        "sentiments = data_frame.Sentiment.values\n",
        "tweets = data_frame.OriginalTweet.values"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cowJr9Oy7wIN"
      },
      "source": [
        "### Data preparation.\n",
        "We are going to prepare our data so that labels will be one hot encoded vectors and we will remove some punctuation marks hashtags for all the tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WyhNek3m6rnM",
        "outputId": "260b4d33-49a1-4fec-d46a-c9c878fe5e51"
      },
      "source": [
        "tweets[2]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Find out how you can protect yourself and loved ones from #coronavirus. ?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8-EWKVNoQ6Q"
      },
      "source": [
        "import re"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "PQxDzx23oUwy",
        "outputId": "2f5c2743-6078-4de5-ba05-18a0f4861cdc"
      },
      "source": [
        "def process_clean_text(text:str)->str:\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)\n",
        "  text = re.sub(r\"[^a-z0-9.,?;']\", ' ', text)\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  return text\n",
        "process_clean_text(tweets[1])"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"when i couldn't find hand sanitizer at fred meyer, i turned to amazon. but 114.97 for a 2 pack of purell?? check out how coronavirus concerns are driving up prices. \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl2UUIVYvKTt"
      },
      "source": [
        "### Label Processing.\n",
        "\n",
        "The following are labels that we have in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2JlbECiuErx",
        "outputId": "2e59a698-5d0b-46d5-fbc5-2f733125d39b"
      },
      "source": [
        "from collections import Counter\n",
        "counts = Counter(sentiments)\n",
        "counts"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'Extremely Negative': 592,\n",
              "         'Extremely Positive': 599,\n",
              "         'Negative': 1041,\n",
              "         'Neutral': 619,\n",
              "         'Positive': 947})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrDnvbzSvpBq"
      },
      "source": [
        "### Visualizing labels using PrettyTable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwUh8xCy6rkk"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "def tabulate(column_names, data, title):\n",
        "  table = PrettyTable(column_names)\n",
        "  table.title = title\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmXv8vgYv8DK",
        "outputId": "ee75c3ec-be23-408f-eb47-1d083bf94440"
      },
      "source": [
        "data_rows = []\n",
        "for label, count in counts.items():\n",
        "  data_rows.append([label.upper(), count])\n",
        "data_columns = [\"LABEL\", \"COUNTS\"]\n",
        "title = \"LABELS COUNTS\"\n",
        "tabulate(data_columns, data_rows, title )"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------+\n",
            "|        LABELS COUNTS        |\n",
            "+--------------------+--------+\n",
            "|       LABEL        | COUNTS |\n",
            "+--------------------+--------+\n",
            "| EXTREMELY NEGATIVE |  592   |\n",
            "|      POSITIVE      |  947   |\n",
            "| EXTREMELY POSITIVE |  599   |\n",
            "|      NEGATIVE      |  1041  |\n",
            "|      NEUTRAL       |  619   |\n",
            "+--------------------+--------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REtgZVWixCQg"
      },
      "source": [
        "### Let's process the labels.\n",
        "* We are going to convert labels to numeric\n",
        "* We are also going to `one_hot` encode labels using `scikit-learn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32scxMKDxgpm"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_aoGKaXxnUC"
      },
      "source": [
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(sentiments)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG0NRZ7wzCGV"
      },
      "source": [
        "### Now the labels are looking as follows:\n",
        "\n",
        "```\n",
        "['Extremely Negative', 'Extremely Positive', 'Negative', 'Neutral', \"Positive\"] == [0, 1, 2, 3, 4]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXUHgsLyz71u"
      },
      "source": [
        "def one_hot_encode(index, depth=5):\n",
        "  return np.eye(depth, dtype=\"float32\")[index]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFd8Lypm0OOO"
      },
      "source": [
        "labels_one_hot = np.array(list(map(one_hot_encode, encoded_labels)))"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LenKwoKC2DVK"
      },
      "source": [
        "### Text (tweets).\n",
        "Now let's map for all the features and get the cleaned version of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yQWqnYF12n2"
      },
      "source": [
        "tweets_cleaned = list(map(process_clean_text, tweets))"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw3Nvp2B2o1G"
      },
      "source": [
        "### Spliting datasets.\n",
        "\n",
        "We are going to split the data into 3 sets:\n",
        "* train `90%` (validation 10% + training 80%)\n",
        "* test `10%`\n",
        "* validation (validation during training) using the `validation_split`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaCkaeTD12lU",
        "outputId": "75fec33f-685c-4048-97a9-56964f116688"
      },
      "source": [
        "test_size = int(.1 * len(tweets_cleaned))\n",
        "test_features = tweets_cleaned[:test_size]\n",
        "test_labels = labels_one_hot[:test_size]\n",
        "\n",
        "train_features = tweets_cleaned[test_size:]\n",
        "train_labels = labels_one_hot[test_size:]\n",
        "\n",
        "\n",
        "data_columns = [\"SET\", \"EXAMPLE(s)\"]\n",
        "title = \"LABELS COUNTS\"\n",
        "data_rows = [\"TESTING\", len(test_labels)], [\"TRAINING\", len(train_labels)]\n",
        "tabulate(data_columns, data_rows, title )\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------+\n",
            "|     LABELS COUNTS     |\n",
            "+----------+------------+\n",
            "|   SET    | EXAMPLE(s) |\n",
            "+----------+------------+\n",
            "| TESTING  |    379     |\n",
            "| TRAINING |    3419    |\n",
            "+----------+------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NAlF4iwC_gG"
      },
      "source": [
        "### Processing the text (features).\n",
        "* Create a word vocabulary.\n",
        "* Create `stoi` from each sentence.\n",
        "* pad the sentences so that they will have the same size.\n",
        "\n",
        "* We are going to join the `train` and `validation` features and labels, and then we will split them during training.\n",
        "\n",
        "**We are not going to touch the test data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "615YPxEWC_b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "915da81d-7233-480a-bbe6-887b2359f75f"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve0gN3_VC_Zj",
        "outputId": "8420ee9a-ab21-42fa-9000-40eacb78f0fe"
      },
      "source": [
        "counter = Counter()\n",
        "for sent in train_features:\n",
        "  words = word_tokenize(sent)\n",
        "  for word in words:\n",
        "    counter[word] += 1\n",
        "\n",
        "counter.most_common(9)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 4231),\n",
              " ('the', 3854),\n",
              " ('to', 3367),\n",
              " (',', 3128),\n",
              " ('?', 2314),\n",
              " ('covid', 2231),\n",
              " ('and', 2191),\n",
              " ('19', 2131),\n",
              " ('of', 1861)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4O30HxWOMKy"
      },
      "source": [
        "### Vocabulary size (aka) number of unique words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9sbL8tdC_Wd",
        "outputId": "e128f363-49b5-4566-c4c5-3edff89db581"
      },
      "source": [
        "vocab_size = len(counter)\n",
        "print(f\"Vocabulary size: {vocab_size:,}\")"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 11,350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAZQl6w2OhL0"
      },
      "source": [
        "### Creating word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuPLINFdC_Ti"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKLNiFInC_QU"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(train_features)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEBCExtqC_NM"
      },
      "source": [
        "word_indices = tokenizer.word_index\n",
        "word_indices_reversed = dict([(v, k) for (k, v) in word_indices.items()])"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IscmPgMpSFoM"
      },
      "source": [
        "### Helper functions.\n",
        "\n",
        "We are going to create two helper function. One will convert the text given to sequences and the other will take sequences and convert them to text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em8lTdRm_ypM"
      },
      "source": [
        "def sequence_to_text(sequences):\n",
        "    return \" \".join(word_indices_reversed[i] for i in sequences)\n",
        "def text_to_sequence(sent):\n",
        "  words = word_tokenize(sent.lower())\n",
        "  sequences = []\n",
        "  for word in words:\n",
        "    try:\n",
        "      sequences.append(word_indices[word])\n",
        "    except:\n",
        "      sequences.append(0)\n",
        "  return sequences"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx8TgBAiTdLT"
      },
      "source": [
        "### Loading pretrainned weights glove.6B.\n",
        "We are going to load this pretrained weights from our google drive. I've uploaded them on my google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zek9CrB1TOBO"
      },
      "source": [
        "embedding_path = \"/content/drive/MyDrive/NLP Data/glove.6B/glove.6B.100d.txt\""
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoC7Xlly7pO6"
      },
      "source": [
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6LrBcKsTcuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd5493b-3908-44a2-b654-89c2194698d7"
      },
      "source": [
        "embeddings_dictionary = dict()\n",
        "start = time.time()\n",
        "with open(embedding_path, encoding='utf8') as glove_file:\n",
        "    for line in glove_file:\n",
        "        records = line.split()\n",
        "        word  = records[0]\n",
        "        vectors = np.asarray(records[1:], dtype='float32')\n",
        "        embeddings_dictionary[word] = vectors\n",
        "\n",
        "print(f\"ETA: {hms_string(time.time() - start)}\")"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ETA: 0:00:09.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5Vs2gHHUXOF"
      },
      "source": [
        "> Creating an `embedding matrix` that suits our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muB0PIgiTcr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "399a7dbe-a7a5-49fd-a418-ac54cc5fb08a"
      },
      "source": [
        "start = time.time()\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    vector = embeddings_dictionary.get(word)\n",
        "    if vector is not None:\n",
        "      try:\n",
        "        embedding_matrix[index] = vector\n",
        "      except:\n",
        "        pass\n",
        "print(f\"ETA: {hms_string(time.time() - start)}\")"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ETA: 0:00:00.02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_My9Wf8WoUQ"
      },
      "source": [
        "### Creating sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFA_nBkTTcoM"
      },
      "source": [
        "sequence_tokens = tokenizer.texts_to_sequences(train_features)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yGvx2PZeTcjt",
        "outputId": "0833ced0-cda2-4a1c-aaaf-3a65240b5d4a"
      },
      "source": [
        "sequence_to_text(sequence_tokens[0])"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"why is toilet paper so important for coronavirus if i'm stuck at home i'm going to stock up on food so i can fuckin eat\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtB8FDOgW2Mv"
      },
      "source": [
        "### Padding sequences.\n",
        "We now want our sequences to have the same size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHisHjeuV69a"
      },
      "source": [
        "max_words = 100\n",
        "tokens_sequence_padded = pad_sequences(sequence_tokens, maxlen=max_words, padding=\"post\", truncating=\"post\")"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW_mPvD2XVCN"
      },
      "source": [
        "### Building the model.\n",
        "\n",
        "### Model achitecture.\n",
        "\n",
        "```\n",
        "                [ Embedding Layer]\n",
        "                        |\n",
        "                        |\n",
        "[ LSTM ] <---- [Bidirectional Layer] ----> [GRU] (forward_layer)\n",
        " (backward_layer)       |\n",
        "                        |\n",
        "        [  Gated Recurrent Unit  (GRU)  ]\n",
        "                        |\n",
        "                        |\n",
        "        [ Long Short Term Memory (LSTM) ]\n",
        "                        |\n",
        "                        |\n",
        "                [ Flatten Layer]\n",
        "                        |\n",
        "                        |\n",
        "                 [Dense Layer 1]\n",
        "                        |\n",
        "                        | \n",
        "                   [ Dropout ]\n",
        "                        |\n",
        "                        |   \n",
        "                 [Dense Layer 2]\n",
        "                        |\n",
        "                        |\n",
        "                 [Dense Layer 3] (output [6 classes])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2_IPav4XBUm",
        "outputId": "b6d7ac5e-f415-4684-e861-b132a5534675"
      },
      "source": [
        "forward_layer = keras.layers.GRU(128, return_sequences=True, dropout=.5 )\n",
        "backward_layer = keras.layers.LSTM(128, activation='tanh', return_sequences=True,\n",
        "                       go_backwards=True, dropout=.5)\n",
        "input_layer = keras.layers.Input(shape=(100, ), name=\"input_layer\")\n",
        "\n",
        "embedding_layer = keras.layers.Embedding(\n",
        "      vocab_size, \n",
        "      100, \n",
        "      input_length=max_words,\n",
        "      weights=[embedding_matrix], \n",
        "      trainable=True,\n",
        "      name = \"embedding_layer\"\n",
        ")(input_layer)\n",
        "bidirectional_layer = keras.layers.Bidirectional(\n",
        "    forward_layer,\n",
        "    backward_layer = backward_layer,\n",
        "    name= \"bidirectional_layer\"\n",
        ")(embedding_layer)\n",
        "\n",
        "gru_layer = keras.layers.GRU(\n",
        "    512, return_sequences=True,\n",
        "   dropout=.5,\n",
        "    name= \"gru_layer\"\n",
        ")(bidirectional_layer)\n",
        "lstm_layer = keras.layers.LSTM(\n",
        "    512, return_sequences=True,\n",
        "    dropout=.5,\n",
        "    name=\"lstm_layer\"\n",
        ")(gru_layer)\n",
        "conv_layer_1 = keras.layers.Conv1D(64, 3, activation='relu')(gru_layer)\n",
        "conv_layer_2 = keras.layers.Conv1D(512, 3, activation='relu')(conv_layer_1)\n",
        "flatten_layer = keras.layers.Flatten(name=\"flatten_layer\")(conv_layer_2)\n",
        "fc_1 = keras.layers.Dense(64, activation='relu', name=\"dense_layer_1\")(flatten_layer)\n",
        "dropout_layer = keras.layers.Dropout(rate=0.5, name=\"dropout_layer\")(fc_1)\n",
        "fc_2 = keras.layers.Dense(512, activation='relu', name=\"dense_layer_2\")(dropout_layer)\n",
        "output_layer = keras.layers.Dense(5, activation='softmax')(fc_2)\n",
        "covid_tweets_model = keras.Model(inputs=input_layer, outputs=output_layer, name=\"covid_tweets_model\")\n",
        "covid_tweets_model.summary()"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"covid_tweets_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_layer (InputLayer)     [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_layer (Embedding)  (None, 100, 100)          1135000   \n",
            "_________________________________________________________________\n",
            "bidirectional_layer (Bidirec (None, 100, 256)          205568    \n",
            "_________________________________________________________________\n",
            "gru_layer (GRU)              (None, 100, 512)          1182720   \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 98, 64)            98368     \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 96, 512)           98816     \n",
            "_________________________________________________________________\n",
            "flatten_layer (Flatten)      (None, 49152)             0         \n",
            "_________________________________________________________________\n",
            "dense_layer_1 (Dense)        (None, 64)                3145792   \n",
            "_________________________________________________________________\n",
            "dropout_layer (Dropout)      (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_layer_2 (Dense)        (None, 512)               33280     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 5)                 2565      \n",
            "=================================================================\n",
            "Total params: 5,902,109\n",
            "Trainable params: 5,902,109\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E_tEILobB-l"
      },
      "source": [
        "### Compiling and training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VaKv9tObB2I"
      },
      "source": [
        "early_stoping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0,\n",
        "    patience=5,\n",
        "    verbose=1,\n",
        "    mode='auto',\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        ")\n",
        "covid_tweets_model.compile(\n",
        "    loss = keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "    optimizer = keras.optimizers.Adam(1e-3, 0.5),\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fH8T6U0ibBxi",
        "outputId": "8907d52d-6d92-40aa-cf72-a2a4ca878330"
      },
      "source": [
        "covid_tweets_model.fit(\n",
        "    tokens_sequence_padded,\n",
        "    train_labels,\n",
        "    epochs = 10,\n",
        "    verbose = 1,\n",
        "    validation_split = .2,\n",
        "    shuffle=True,\n",
        "    batch_size= 32,\n",
        "    validation_batch_size = 16,\n",
        "    callbacks = [early_stoping]\n",
        ")"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "86/86 [==============================] - 8s 53ms/step - loss: 1.5815 - accuracy: 0.2713 - val_loss: 1.5482 - val_accuracy: 0.2529\n",
            "Epoch 2/10\n",
            "86/86 [==============================] - 4s 42ms/step - loss: 1.5460 - accuracy: 0.2896 - val_loss: 1.5408 - val_accuracy: 0.2661\n",
            "Epoch 3/10\n",
            "86/86 [==============================] - 4s 42ms/step - loss: 1.5240 - accuracy: 0.3002 - val_loss: 1.5072 - val_accuracy: 0.3070\n",
            "Epoch 4/10\n",
            "86/86 [==============================] - 4s 42ms/step - loss: 1.5111 - accuracy: 0.3068 - val_loss: 1.5245 - val_accuracy: 0.2675\n",
            "Epoch 5/10\n",
            "86/86 [==============================] - 4s 42ms/step - loss: 1.4980 - accuracy: 0.3144 - val_loss: 1.5034 - val_accuracy: 0.2865\n",
            "Epoch 6/10\n",
            "86/86 [==============================] - 4s 42ms/step - loss: 1.4496 - accuracy: 0.3499 - val_loss: 1.5830 - val_accuracy: 0.2909\n",
            "Epoch 7/10\n",
            "86/86 [==============================] - 4s 42ms/step - loss: 1.4120 - accuracy: 0.3590 - val_loss: 1.4929 - val_accuracy: 0.3450\n",
            "Epoch 8/10\n",
            "86/86 [==============================] - 4s 42ms/step - loss: 1.3540 - accuracy: 0.4102 - val_loss: 1.7932 - val_accuracy: 0.3246\n",
            "Epoch 9/10\n",
            "86/86 [==============================] - 4s 42ms/step - loss: 1.2894 - accuracy: 0.4450 - val_loss: 1.3877 - val_accuracy: 0.3947\n",
            "Epoch 10/10\n",
            "86/86 [==============================] - 4s 42ms/step - loss: 1.2091 - accuracy: 0.4830 - val_loss: 1.3799 - val_accuracy: 0.3611\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff899d7c990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMjQhhzqB8NT"
      },
      "source": [
        "### BERT - TEXT CLASSIFICATION\n",
        "As we can see that our model is not performing well, it is not improving from the achitecture that works perfectly from the `emotionals-nlp-notebook`. We are going to use transfare learning to get reasonable accuracy for this task. Specifically we are going to use the `BERT` model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFGxLEJsDSg6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji7Jjeuafxov"
      },
      "source": [
        "### Evaluating the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVJCnBxRf0VL",
        "outputId": "1a946def-b66a-458a-d339-c1eb38385091"
      },
      "source": [
        "def text_to_padded_sequences(sent):\n",
        "  tokens = text_to_sequence(sent)\n",
        "  padded_tokens = pad_sequences([tokens], maxlen=max_words, padding=\"post\", truncating=\"post\")\n",
        "  return tf.squeeze(padded_tokens)\n",
        "\n",
        "X_test = np.array(list(map(text_to_padded_sequences, X_test_values)))\n",
        "emotion_model.evaluate(X_test, y_test_labels_one_hot, verbose=1, batch_size=32)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 2s 20ms/step - loss: 0.1375 - accuracy: 0.9335\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.13748641312122345, 0.9334999918937683]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JGdHvW1cCB1"
      },
      "source": [
        "### Inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo8lPGu2qrbN"
      },
      "source": [
        "def tabulate(column_names, data):\n",
        "  table = PrettyTable(column_names)\n",
        "  table.align[column_names[0]] = \"l\"\n",
        "  table.align[column_names[1]] = \"l\"\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "\n",
        "  print(table.get_string(title=\"EMOTION PREDICTIONS TABLE\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfTD8ynWbBuX"
      },
      "source": [
        "def predict(model, sent):\n",
        "    classes = ['anger', 'fear', 'joy', 'love', 'sadness', 'surprise' ]\n",
        "    tokens = text_to_sequence(sent)\n",
        "    padded_tokens = pad_sequences([tokens], maxlen=max_words, padding=\"post\", truncating=\"post\")\n",
        "    probabilities = model.predict(padded_tokens)\n",
        "    prediction = tf.argmax(probabilities, axis=1).numpy()[0]\n",
        "    class_name = classes[prediction]\n",
        "    emoji_text = emoji.emojize(emotions_emojis[class_name], language='en', use_aliases=True)\n",
        "    \n",
        "    table_headers =[\"KEY\", \"VALUE\"]\n",
        "    table_data = [\n",
        "        [\"PREDICTED CLASS\",  prediction],\n",
        "        [\"PREDICTED CLASS NAME\",  class_name],\n",
        "        [\"PREDICTED CLASS EMOJI\",  emoji_text],\n",
        "        [\"CONFIDENCE OVER OTHER CLASSES\", f'{probabilities[0][prediction] * 100:.2f}%']       \n",
        "    ]\n",
        "    tabulate(table_headers, table_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMmn3HgchODO"
      },
      "source": [
        "### Sadness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di_Xaqp0bBrf",
        "outputId": "9c838888-864f-43d4-b818-c9ca875614b9"
      },
      "source": [
        "predict(emotion_model, \"im updating my blog because i feel shitty.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------+\n",
            "|        EMOTION PREDICTIONS TABLE        |\n",
            "+-------------------------------+---------+\n",
            "| KEY                           | VALUE   |\n",
            "+-------------------------------+---------+\n",
            "| PREDICTED CLASS               | 4       |\n",
            "| PREDICTED CLASS NAME          | sadness |\n",
            "| PREDICTED CLASS EMOJI         | 😞      |\n",
            "| CONFIDENCE OVER OTHER CLASSES | 99.65%  |\n",
            "+-------------------------------+---------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDoKLPbrhShU"
      },
      "source": [
        "### Fear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWLTsgoWbBp5",
        "outputId": "7e8d1acd-28d1-41a2-d0e5-4fadf72755ea"
      },
      "source": [
        "predict(emotion_model, \"i am feeling apprehensive about it but also wildly excited\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------+\n",
            "|        EMOTION PREDICTIONS TABLE        |\n",
            "+-------------------------------+---------+\n",
            "| KEY                           | VALUE   |\n",
            "+-------------------------------+---------+\n",
            "| PREDICTED CLASS               | 1       |\n",
            "| PREDICTED CLASS NAME          | fear    |\n",
            "| PREDICTED CLASS EMOJI         | 😨      |\n",
            "| CONFIDENCE OVER OTHER CLASSES | 100.00% |\n",
            "+-------------------------------+---------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGmhhc2hhe2E"
      },
      "source": [
        "### Joy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROGMsOAkbBlW",
        "outputId": "e5e5d80b-535a-4d05-a1e7-b7bd7c604394"
      },
      "source": [
        "predict(emotion_model, \"i feel a little mellow today.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------+\n",
            "|        EMOTION PREDICTIONS TABLE        |\n",
            "+-------------------------------+---------+\n",
            "| KEY                           | VALUE   |\n",
            "+-------------------------------+---------+\n",
            "| PREDICTED CLASS               | 2       |\n",
            "| PREDICTED CLASS NAME          | joy     |\n",
            "| PREDICTED CLASS EMOJI         | 😄      |\n",
            "| CONFIDENCE OVER OTHER CLASSES | 100.00% |\n",
            "+-------------------------------+---------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldyKnuyCh81u"
      },
      "source": [
        "### Surprise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxiWcolKheke",
        "outputId": "0b6d85c4-4325-40da-e83b-8534a6aac8f0"
      },
      "source": [
        "predict(emotion_model, \"i feel shocked and sad at the fact that there are so many sick people.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------+\n",
            "|        EMOTION PREDICTIONS TABLE         |\n",
            "+-------------------------------+----------+\n",
            "| KEY                           | VALUE    |\n",
            "+-------------------------------+----------+\n",
            "| PREDICTED CLASS               | 5        |\n",
            "| PREDICTED CLASS NAME          | surprise |\n",
            "| PREDICTED CLASS EMOJI         | 😮       |\n",
            "| CONFIDENCE OVER OTHER CLASSES | 99.97%   |\n",
            "+-------------------------------+----------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMRVPZckiDQH"
      },
      "source": [
        "### Love"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7ZR6hRybBiH",
        "outputId": "47721995-2b6f-45fd-9c92-b172368fbd85"
      },
      "source": [
        "predict(emotion_model, \"i want each of you to feel my gentle embrace.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------+\n",
            "|       EMOTION PREDICTIONS TABLE        |\n",
            "+-------------------------------+--------+\n",
            "| KEY                           | VALUE  |\n",
            "+-------------------------------+--------+\n",
            "| PREDICTED CLASS               | 3      |\n",
            "| PREDICTED CLASS NAME          | love   |\n",
            "| PREDICTED CLASS EMOJI         | 😍     |\n",
            "| CONFIDENCE OVER OTHER CLASSES | 97.07% |\n",
            "+-------------------------------+--------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxLlCtn-iUPr"
      },
      "source": [
        "### Anger."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBL9swhHhpM1",
        "outputId": "9321bcdc-97b5-498b-dc5d-4c58edd00b25"
      },
      "source": [
        "predict(emotion_model, \"i feel like my irritable sensitive combination skin has finally met it s match.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------+\n",
            "|        EMOTION PREDICTIONS TABLE        |\n",
            "+-------------------------------+---------+\n",
            "| KEY                           | VALUE   |\n",
            "+-------------------------------+---------+\n",
            "| PREDICTED CLASS               | 0       |\n",
            "| PREDICTED CLASS NAME          | anger   |\n",
            "| PREDICTED CLASS EMOJI         | 😠      |\n",
            "| CONFIDENCE OVER OTHER CLASSES | 100.00% |\n",
            "+-------------------------------+---------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE_obFubiRzg"
      },
      "source": [
        "### Saving the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Abyb05jjixlW",
        "outputId": "bb153f80-c5c7-4c18-ee21-b1e562302c23"
      },
      "source": [
        "emotion_model.save(os.path.join(data_path, \"emotional_model.h5\"))\n",
        "print(\"Model Saved!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Saved!!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}